{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48137405-0f13-477e-a507-fd4e37442afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a2a290e-2920-4620-afb7-5cfac8761d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = PyPDFLoader(\"./Attention.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc58afdc-f42c-4080-ab01-4c48f866cc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = pdf_file.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a86a2c26-468c-440d-be0a-077f72517ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': './Attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9573f709-651f-4edc-8c39-f11d4204d63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1024,\n",
    "    chunk_overlap = 128,    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69e3ed2d-66f6-436a-88a8-aadb2a7d2e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "016dff90-4553-4e81-aa5c-866e0c9d06b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "058e3aff-c1fe-4ce6-b314-2a2dcf50d396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': './Attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a104ea4-dbc4-4835-a215-49a33b5b1ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Embeddings = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                                   model_kwargs = {\"device\": \"cuda\"})\n",
    "\n",
    "vector_db = FAISS.from_texts([str(chunk) for chunk in chunks], Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a377d74e-3b1f-416b-8e6e-565aa1450d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '''\n",
    "what is the purpose of the decoder?\n",
    "'''\n",
    "\n",
    "relevant_results = vector_db.similarity_search(question, k = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c74b4141-31f7-453d-9230-0bbe3e3db7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id='5be55beb-0be1-4e24-9a45-7ea48d0b8e45', metadata={}, page_content='page_content=\\'• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\' metadata={\\'producer\\': \\'pdfTeX-1.40.25\\', \\'creator\\': \\'LaTeX with hyperref\\', \\'creationdate\\': \\'2024-04-10T21:11:43+00:00\\', \\'author\\': \\'\\', \\'keywords\\': \\'\\', \\'moddate\\': \\'2024-04-10T21:11:43+00:00\\', \\'ptex.fullbanner\\': \\'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5\\', \\'subject\\': \\'\\', \\'title\\': \\'\\', \\'trapped\\': \\'/False\\', \\'source\\': \\'./Attention.pdf\\', \\'total_pages\\': 15, \\'page\\': 4, \\'page_label\\': \\'5\\'}')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a531f9e7-2b9e-405f-9c30-c7047f821bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "Using this piece of information:\n",
    "\\n\n",
    "{context}\n",
    "\\n\n",
    "Answer the following question:\n",
    "\\n\n",
    "{question}\n",
    "\\n\n",
    "Answer:\n",
    "\\n \n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4af95ed-c2b3-4fbe-9168-f51278dd17d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = \"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c712e8c-84a0-4b48-822f-0728e778ce5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-3b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-1b1\", quantization_config = bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5cd41ed5-8258-4f3f-aacb-0be3eeaf406c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model = model, tokenizer = tokenizer, max_new_tokens = 144)\n",
    "lc_pipeline = HuggingFacePipeline(pipeline = pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1ab5d829-2331-4f5a-b919-1730d7ba968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm = lc_pipeline,\n",
    "    retriever = vector_db.as_retriever(search_kwargs = {'k':3}),\n",
    "    return_source_documents = True,\n",
    "    chain_type_kwargs = {'prompt' : prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cda9acc4-4912-4793-ba0d-dd1083623c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '''\n",
    "what is the purpose of the decoder?\n",
    "'''\n",
    "\n",
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "166e0b24-d88c-43ac-a7cf-be639631cf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " \n",
      "The purpose of the decoder is to predict the next position in the input sequence. This is done by\n",
      "attending to the output of the previous decoder layer. The decoder is a stack of N = 6\n",
      "identical layers, each of which contains a self-attention layer. The decoder stack is\n",
      "implemented using a residual connection around each of the two sub-layers, followed by\n",
      "layer normalization. The decoder stack contains a single self-attention layer, which is\n",
      "implemented using a residual connection around each of the two sub-l\n"
     ]
    }
   ],
   "source": [
    "print(result[\"result\"].split(\"Answer:\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "30512d31-947d-4059-b33f-4ef3b5834eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " \n",
      "The attention function is the softmax function of the query with the corresponding key.\n",
      "The attention function is computed by a compatibility function of the query with the\n",
      "corresponding key.\n",
      "The attention function is computed by a compatibility function of the query with the\n",
      "corresponding key.\n",
      "The attention function is computed by a compatibility function of the query with the\n",
      "corresponding key.\n",
      "The attention function is computed by a compatibility function of the query with the\n",
      "corresponding key.\n",
      "The attention function is computed by a compatibility function of the query with the\n",
      "correspond\n"
     ]
    }
   ],
   "source": [
    "question = '''\n",
    "what is the attention function?\n",
    "'''\n",
    "\n",
    "result = qa_chain({\"query\": question})\n",
    "print(result[\"result\"].split(\"Answer:\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "337ceeb9-3172-4569-9b9e-a4415e6cb6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " \n",
      "1. The first way is to use the attention mechanism to attend to the\n",
      "information from the previous decoder layer. This is the most common way of using\n",
      "attention in the WMT 2014 dataset. The attention mechanism is used to attend to the\n",
      "information from the previous decoder layer, and the attention head is used to attend to\n",
      "the information from the next decoder layer. The attention mechanism is used to attend to\n",
      "the information from the previous decoder layer, and the attention head is used to attend to\n",
      "the information from the next decoder\n"
     ]
    }
   ],
   "source": [
    "question = '''\n",
    "what are the three ways of using multi-head attention?\n",
    "'''\n",
    "\n",
    "result = qa_chain({\"query\": question})\n",
    "print(result[\"result\"].split(\"Answer:\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aacc22-8842-495e-a9f2-761d670004be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
